■MirrorShard 2のAI機能について  
　Electron版のMirrorShardには２種類のAI機能を搭載していましたが、現在MirrorShard 2では、AIチャットウィンドウのみが使用可能です。  

# 使い方  

　メインエディタのUIボタン（またはCtrl+Shift+A）で、AIと自由に対話できる専用のチャットウィンドウを開きます。執筆中の相談相手として、キャラクターのロールプレイ相手として、あるいは、TRPGのセッションログのように、AIとの対話そのものを楽しむこともできます。  

## 基本的な使い方  

- メッセージの送信: 画面下部の入力エリアにテキストを入力し、「送信」ボタンかCtrl+Enterを押します。  
- UIボタン（括弧内は対応ショートカット）:  
    - ログのクリア（Ctrl+Shift+C）: 現在のチャットログをクリアします。  
    - 保存（Ctrl+S）: 現在のチャットログを「名前を付けて保存」します。  
    - 別名保存: ファイルに名前をつけて保存します。  
    - 読込（Ctrl+O）: 過去に保存したチャットログ（MirrorShard独自形式, LM Studio形式, Gemini形式に対応）を読み込みます。  
    - 最大化（Win・Linux:F11　Mac:Cmd+Ctrl+F）: ウィンドウを最大化/元に戻します。  
    - 閉じる（Ctrl+Shift+A）: ウィンドウを閉じます。  
- 右クリックメニュー: ウィンドウ内を右クリックすると、ログの保存/読込のほか、コピー/ペーストなどの便利な機能にアクセスできます。
    - チャットログを直接メインエディタに送ることも出来ます。送ったデータは新規テキストファイルとして開かれます。  

## 対話のコントロール  

各メッセージの横に表示されるボタンで、対話を細かくコントロールできます。  
- 編集 : ユーザーのメッセージを編集し、そこからAIの応答を再生成させます。  
- 削除 : そのメッセージ以降の、すべての対話履歴を削除します。  
- 再生成 : AIの応答に満足できない場合、同じ文脈で、もう一度別の応答を生成させます。  
- コピー : AIの応答を、クリップボードにコピーします。  

## 名前とアイコンのカスタマイズ  

　ユーザーとAIの表示名は、設定タブで変更することが出来ます。  
　また、ユーザーやAIにアイコンを設定することもできます。アイコンには任意の画像ファイルを設定することができます。  


# AIの導入方法  

## Geminiの場合  

1. Google AI Studioへのログイン: Googleアカウントで、Google AI Studio にアクセスします。  
2. APIキーの取得:  
　・画面左側の「Get API key」をクリックします。  
　・「Create API key in new project」でAPIキーを生成し、その文字列をコピーしておきます。  
　・※APIキーは他人に知られないよう、厳重にご注意ください。  
3. MirrorShardの設定:  
　・設定ウィンドウで「AI設定」タブを選択し、「API Key」欄にコピーしたAPIキーを貼り付けてください。  

## ローカルAIを使用する場合  

### LM Studioの場合  

1. LM Studioの準備:  
　・LM Studioを起動し、左メニューの「開発者」タブを開きます。  
　・お好きなモデルをロードします。（モデルをお持ちでない場合は、「探索」タブからダウンロードしてください）  
　・左上隅の「Status:Stopped」と書かれているボタンをクリックしてサーバを起動します（「Status: Running」になります）。  
　・その右の「Settings」から「CORSを有効にする」を有効にします。  
　・画面右側のパネルから「Context」タブを選び、システムプロンプトを記入すると、より望む出力結果が得やすくなります。  

２. MirrorShardの設定:  
　・初期設定のままローカルホストで運用する場合には、そのまま（Endpoint URL: http://127.0.0.1:1234/v1/chat/completions のまま）で設定完了です。
　・LANでアクセスする場合にはLM Studio側の設定（Server Settings）で「ローカルネットワークで提供」をオンにし、MirrorShard側の設定ウィンドウで「AI設定」タブを選択しで、「EndpointURL」のIPアドレスをご自身の環境に合わせて書き換えます。  

### Ollamaの場合  

Ollamaはコマンドラインベースですが、軽量で動作が安定しているローカルLLM実行環境です。  

    Ollamaの準備:  
    ・公式サイト ( https://ollama.com/ ) からOllamaをダウンロードしてインストールします。  
    ・ターミナル（PowerShellやコマンドプロンプト）を開き、使いたいモデルをプル（ダウンロード）します。  
    例: ollama pull gemma2 または ollama pull llama3  
    ・Ollamaは通常、バックグラウンドで自動的に起動しています（タスクトレイにアイコンがあります）。  

    MirrorShardの設定:  
    ・設定ウィンドウの「AI設定」タブを開きます。  
    ・Endpoint URL: http://127.0.0.1:11434/v1/chat/completions と入力します。（ポート番号がLM Studioとは異なります）  
    ・Model Name: ターミナルでプルしたモデル名を入力します（例: gemma2 や llama3:latest）。※ここが間違っていると動きません。  
    ・「適用」を押して保存します。  

これでOllamaとの対話が可能になります。  


# AIの選択  

AIチャットウィンドウの左上のプルダウンメニューから、GeminiとローカルAIのどちらかを選択します。  

## Geminiのモデル選択
　設定ウィンドウの「Model」欄の右にあるプルダウンメニューから、gemini-2.5-proとgemini-2.5-flashのいずれかを選択できます。  
「Model」欄にモデル名を直接入力することで、それ以外のバージョンのGeminiを使うことも出来ます（後述）  

## Geminiの長所と短所  
　長所：導入が比較的容易な上に非常に高性能  
　短所：通信に時間がかかるため生成が遅いこと、使い方によっては課金が必要になること、データが学習に用いられる可能性があること、Googleの厳格なポリシーに違反する使い方は出来ないこと  

### Gemini 3.0 Pro などの最新モデルへの対応について  
設定画面の「Model」欄で「手動入力」を選択、モデル名を直接入力することで、gemini-3-pro-preview などの最新モデルを指定することも可能です。  
【重要：料金について】  
デフォルトの選択肢（Gemini 2.5 Pro等）は無料枠（Free Tier）に対応していますが、カスタム設定で指定可能な Gemini 3系などの最新モデルには無料枠が提供されていない場合があります。  
Google Cloudで課金を有効にしているアカウントでこれらのモデルを使用すると、API利用料が発生する可能性があります。ご利用の際はGoogleの料金プランを十分にご確認ください。  

## ローカルAIの長所と短所  
　長所：ローカルで運用できるためデータが学習されないこと、十分なマシンスペック（特にグラフィックボード）があればGeminiに較べてレスポンスがかなり高速なこと、多様なモデルを使い分けられること、課金が不要なこと、ポリシーが緩いこと  
　短所：導入が比較的複雑なこと、Geminiに較べて性能が劣ること、低スペックなマシンではGeminiより却って遅くなること、LM Studio（またはOllama）を常駐させておかなければならないのでメモリ消費量が増えること  


# AI機能使用上の注意  

## ⚠️ AI機能（Gemini使用時）の利用料金について  

AIチャット機能でGeminiを選択した場合、Google社の Gemini API を使用することになります。  
APIの利用料金は、**ユーザーご自身のGoogleアカウントおよびGoogle Cloudプロジェクトの契約状況**に依存します。  

*   **無料枠（Free Tier）での利用:**  
    *   クレジットカード情報を登録していない、または課金設定を無効にしている場合、無料枠の範囲内で利用可能です。  
    *   無料枠の上限に達した場合、APIはエラーを返し、それ以上の利用はできなくなります（勝手に課金されることはありません）。  
*   **従量課金（Pay-as-you-go）での利用:**  
    *   Google Cloudプロジェクトで課金を有効にしている場合、無料枠を超えた利用分や、無料枠の対象外である高性能モデル（Gemini 3系など）の利用に対して、**Google社から料金が請求される可能性があります。**  

**【免責事項】**  
APIの利用に伴い発生した料金について、本ソフトウェア開発者は一切の責任を負いません。  
ご利用の際は、必ず [Google Gemini APIの料金ページ](https://ai.google.dev/pricing) をご確認の上、ご自身の判断でモデルの選択や利用頻度の調整を行ってください。  

## [Gemini APIキーの取り扱いについて]  
・APIキーは、あなたのアカウントに紐づく「秘密の鍵」です。他人に知られないよう、厳重に管理してください。  
・万が一APIキーが第三者に流出した場合、あなたの無料利用枠が不正に消費されたり、予期せぬ問題が発生したりする可能性があります。  

## [AI切り替え時のコンテクスト（文脈）について]  
　AI使用時、チャットの場合はログの全体がコンテクストとしてAIに送られます。  
　ローカルLLMの中にはセンシティブなコンテンツ（暴力行為、NSFWなど）に対する制約の緩いものもありますが、センシティブな内容のコンテクストをうっかりGeminiに送ってしまったりした場合、Googleの利用規約に抵触して思わぬ事態が発生する場合があります。  
　ご利用の際には、接続先が間違っていないか十分ご確認ください。  

## ログビューアとして使う際の注意点  
　本機能はGoogle AI studioやLM Studioで生成したログを閲覧するビューアとして使うことも出来ますが、読み込めるログの長さには限界があり、数十万トークンあるような長大なログを読み込むとフリーズしてしまいます。  
　こうした長大なログを閲覧する際には、AIチャットウィンドウではなくメインエディタの右クリックメニューから「Geminiログをインポート」を選択し、メインエディタで閲覧されることをお勧めいたします（ただしGemini形式のみ対応）。  
　メインエディタの心臓部であるエディタライブラリ「CodeMirror」は巨大テキストの扱いに強く、数十万トークン程度なら問題なく読み込めます。  

## AIの生成した回答が途中で切れてしまう場合  
　AIの応答長を短く設定した場合などに、ローカルAIの返答が途中で切れてしまうことがあります。  
　これは、AIモデルの指示追従能力が低いために字数内で内容を要約することができず、知っている情報をすべて出力しようとしてしまった結果、MirrorShard側の文字数制限に引っかかって途中で回答が終わってしまうことによって発生します。  
　このようなケースでは、ローカルAI側でシステムプロンプトや応答長の設定をすると改善される場合があります。たとえばLM Studioの場合、「開発者」タブの右側にあるパネルで、「Context」タブや「Inference」タブから各種設定を行ってください。  


